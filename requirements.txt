# =================================================
# Project Sunshine Document Extraction Pipeline v2
# Requirements for isolated Linux server with A100 GPU
# =================================================

# Core ML/LLM
torch>=2.0.0
transformers>=4.40.0
accelerate>=0.27.0
bitsandbytes>=0.42.0

# llama-cpp-python: fast GGUF inference (LLM_BACKEND = "llamacpp")
# Install with CUDA support on the A100 server:
#   CMAKE_ARGS="-DGGML_CUDA=on" pip install "llama-cpp-python>=0.2.90"
#
# IMPORTANT â€” llama-cpp-python 0.3.16 (latest as of Aug 2025) bundles a
# llama.cpp that does NOT support newer architectures such as 'mistral3'
# (used by Ministral models released Dec 2025+).
# If you see "unknown model architecture: 'mistral3'" at runtime, build
# llama-cpp-python from source with the latest llama.cpp:
#
#   git clone https://github.com/abetlen/llama-cpp-python.git
#   cd llama-cpp-python
#   git submodule update --init --recursive
#   cd vendor/llama.cpp && git pull origin master && cd ../..
#   CMAKE_ARGS="-DGGML_CUDA=on" pip install -e . --no-cache-dir
llama-cpp-python>=0.2.90

# Document Processing
pymupdf>=1.24.0
python-docx>=1.1.0
openpyxl>=3.1.0
extract-msg>=0.48.0
pandas>=2.0.0

# OCR
easyocr>=1.7.0
Pillow>=10.0.0

# Text Processing
langdetect>=1.0.9
fuzzywuzzy>=0.18.0
python-Levenshtein>=0.25.0
beautifulsoup4>=4.12.0

# Archive Support
py7zr>=0.21.0

# Optional: Flash Attention (for faster inference)
# Uncomment if your CUDA supports it
# flash-attn>=2.5.0
