import os
import zipfile
import py7zr
import shutil
import tempfile
import torch
import pandas as pd
import logging
import re
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import fitz  # PyMuPDF
import easyocr
from langdetect import detect, LangDetectException
from dateutil import parser as date_parser
from collections import Counter
import pkg_resources  # For PyMuPDF version checking
import extract_msg  # For .msg files
from docx import Document  # For .docx files
import subprocess  # For checking and using catdoc
import email  # Built-in library for parsing .eml files
import gc  # For garbage collection
import argparse  # For command-line arguments

# Logging Setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration
MODEL_PATH = "/home/inghero/data/dqanalysis/llm/parc/qwen3_14B"
ARCHIVE_PATH = "/home/inghero/data/dqanalysis/llm/parc/notebooks/LGD/input/sales_conf.7z"
OUTPUT_CSV = "/home/inghero/data/dqanalysis/llm/parc/notebooks/LGD/output/sales_info_2.csv"
INTERMEDIATE_CSV = "/home/inghero/data/dqanalysis/llm/parc/notebooks/LGD/output/intermediate_results.csv"
EXTRACTED_TEXT_DIR = "extracted_texts"  # Directory to save extracted texts
CHUNK_OUTPUTS_DIR = "chunk_outputs"  # Directory to save chunk outputs for debugging
BATCH_SIZE = 2  # Further reduced to avoid GPU memory errors
DPI = 400  # Increased DPI for better OCR accuracy

# Ensure directories exist
for directory in [EXTRACTED_TEXT_DIR, CHUNK_OUTPUTS_DIR]:
    if not os.path.exists(directory):
        os.makedirs(directory)

# Check if catdoc is installed
try:
    subprocess.check_call(['which', 'catdoc'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    logger.info("catdoc is installed. .doc files will be processed using catdoc.")
except subprocess.CalledProcessError:
    logger.warning("catdoc is not installed. .doc files will be skipped. Consider installing catdoc or converting .doc files to .docx manually.")

# Utility Functions for OCR
def check_pymupdf_version():
    try:
        pymupdf_version = pkg_resources.get_distribution("pymupdf").version
        logger.info(f"PyMuPDF version: {pymupdf_version}")
        return pymupdf_version
    except Exception as e:
        logger.error(f"Error checking PyMuPDF version: {e}")
        return None

def verify_easyocr_models(model_dir="~/.EasyOCR/model"):
    model_dir = os.path.expanduser(model_dir)
    required_models = ['craft_mlt_25k.pth', 'latin_g2.pth']
    missing_models = [model for model in required_models if not os.path.exists(os.path.join(model_dir, model))]
    if missing_models:
        logger.error(f"Missing EasyOCR models: {', '.join(missing_models)} in {model_dir}")
        return False
    logger.info(f"All required EasyOCR models found in {model_dir}")
    return True

# Helper Functions for Text Extraction
def ocr_page(page, reader, dpi):
    try:
        pix = page.get_pixmap(dpi=dpi)
        img_path = f"temp_page_{page.number}.png"
        pix.save(img_path)
        result = reader.readtext(img_path)
        text = "\n".join([res[1] for res in result])
        os.remove(img_path)
        return text
    except Exception as e:
        logger.error(f"OCR error for page {page.number}: {e}")
        return ""

def process_page(page, reader, dpi, text_threshold=50):
    try:
        if page.rotation != 0:
            page.set_rotation(0)
        txt = page.get_text("text")
        if len(txt.strip()) >= text_threshold:
            logger.info(f"Page {page.number}: Using direct text extraction.")
            return txt
        else:
            logger.info(f"Page {page.number}: Using OCR.")
            return ocr_page(page, reader, dpi)
    except Exception as e:
        logger.error(f"Error processing page {page.number}: {e}")
        return ""

def extract_text_from_pdf(pdf_path, languages=['en', 'nl', 'fr'], dpi=DPI, text_threshold=50):
    full_text = ""
    try:
        check_pymupdf_version()
        if not verify_easyocr_models():
            logger.warning(f"EasyOCR models missing. OCR will not be performed for {pdf_path}.")
        doc = fitz.open(pdf_path)
        if doc.is_form_pdf:
            logger.info(f"Detected XFA PDF: {pdf_path}. Attempting OCR for all pages.")
            if verify_easyocr_models():
                reader = easyocr.Reader(languages)
                for page_num in range(len(doc)):
                    page = doc.load_page(page_num)
                    text = ocr_page(page, reader, dpi)
                    full_text += text + "\n\n"
            else:
                logger.warning(f"Cannot perform OCR for XFA PDF {pdf_path} due to missing models.")
        else:
            reader = easyocr.Reader(languages) if verify_easyocr_models() else None
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                text = process_page(page, reader, dpi, text_threshold) if reader else page.get_text("text")
                full_text += text + "\n\n"
        doc.close()
        logger.info(f"Extracted text from {pdf_path} ({len(full_text.split())} words).")
        text_file_name = os.path.splitext(os.path.basename(pdf_path))[0] + ".txt"
        text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
        with open(text_file_path, 'w', encoding='utf-8') as f:
            f.write(full_text)
        logger.info(f"Saved extracted text to {text_file_path}")
    except Exception as e:
        logger.error(f"Error processing {pdf_path}: {e}")
    return full_text if full_text else None

def extract_text_from_xls(xls_path):
    try:
        df = pd.read_excel(xls_path, sheet_name=None)
        full_text = ""
        for sheet_name, sheet_df in df.items():
            full_text += sheet_df.to_string() + "\n\n"
        logger.info(f"Extracted text from {xls_path} ({len(full_text.split())} words).")
        text_file_name = os.path.splitext(os.path.basename(xls_path))[0] + ".txt"
        text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
        with open(text_file_path, 'w', encoding='utf-8') as f:
            f.write(full_text)
        logger.info(f"Saved extracted text to {text_file_path}")
        return full_text
    except Exception as e:
        logger.error(f"XLS extraction error: {e}")
        return None

def extract_text_from_docx(docx_path):
    try:
        doc = Document(docx_path)
        text = "\n".join([para.text for para in doc.paragraphs])
        logger.info(f"Extracted text from {docx_path} ({len(text.split())} words).")
        text_file_name = os.path.splitext(os.path.basename(docx_path))[0] + ".txt"
        text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
        with open(text_file_path, 'w', encoding='utf-8') as f:
            f.write(text)
        logger.info(f"Saved extracted text to {text_file_path}")
        return text
    except Exception as e:
        logger.error(f"DOCX extraction error: {e}")
        return None

def extract_text_from_txt(txt_path):
    try:
        with open(txt_path, 'r', encoding='utf-8') as f:
            text = f.read()
        logger.info(f"Extracted text from {txt_path} ({len(text.split())} words).")
        text_file_name = os.path.splitext(os.path.basename(txt_path))[0] + ".txt"
        text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
        with open(text_file_path, 'w', encoding='utf-8') as f:
            f.write(text)
        logger.info(f"Saved extracted text to {text_file_path}")
        return text
    except Exception as e:
        logger.error(f"TXT extraction error: {e}")
        return None

def extract_text_from_csv(csv_path):
    try:
        df = pd.read_csv(csv_path)
        text = df.to_string(index=False)
        logger.info(f"Extracted text from {csv_path} ({len(text.split())} words).")
        text_file_name = os.path.splitext(os.path.basename(csv_path))[0] + ".txt"
        text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
        with open(text_file_path, 'w', encoding='utf-8') as f:
            f.write(text)
        logger.info(f"Saved extracted text to {text_file_path}")
        return text
    except Exception as e:
        logger.error(f"CSV extraction error: {e}")
        return None

def extract_text_from_doc(doc_path):
    try:
        # Check if catdoc is installed
        subprocess.check_call(['which', 'catdoc'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        # Use catdoc to extract text
        text = subprocess.check_output(['catdoc', doc_path]).decode('utf-8')
        logger.info(f"Extracted text from {doc_path} using catdoc ({len(text.split())} words).")
        text_file_name = os.path.splitext(os.path.basename(doc_path))[0] + ".txt"
        text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
        with open(text_file_path, 'w', encoding='utf-8') as f:
            f.write(text)
        logger.info(f"Saved extracted text to {text_file_path}")
        return text
    except subprocess.CalledProcessError:
        logger.warning(f"catdoc not found. Skipping {doc_path}. Consider installing catdoc or converting to .docx manually.")
        return None
    except Exception as e:
        logger.error(f"DOC extraction error: {e}")
        return None

def extract_text_from_eml(eml_path):
    try:
        with open(eml_path, 'r', encoding='utf-8') as f:
            msg = email.message_from_file(f)
        text = ""
        if msg.is_multipart():
            for part in msg.walk():
                content_type = part.get_content_type()
                if content_type == 'text/plain':
                    text += part.get_payload(decode=True).decode('utf-8', errors='ignore')
                elif content_type == 'text/html':
                    text += part.get_payload(decode=True).decode('utf-8', errors='ignore')
        else:
            text = msg.get_payload(decode=True).decode('utf-8', errors='ignore')
        logger.info(f"Extracted text from {eml_path} ({len(text.split())} words).")
        text_file_name = os.path.splitext(os.path.basename(eml_path))[0] + ".txt"
        text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
        with open(text_file_path, 'w', encoding='utf-8') as f:
            f.write(text)
        logger.info(f"Saved extracted text to {text_file_path}")
        return text
    except Exception as e:
        logger.error(f"EML extraction error: {e}")
        return None

def process_msg_file(msg_path, temp_dir):
    try:
        msg = extract_msg.Message(msg_path)
        texts = []
        if msg.body:
            texts.append(("body", msg.body))
            text_file_name = os.path.splitext(os.path.basename(msg_path))[0] + "_body.txt"
            text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
            with open(text_file_path, 'w', encoding='utf-8') as f:
                f.write(msg.body)
            logger.info(f"Saved extracted text to {text_file_path}")
        for attachment in msg.attachments:
            if attachment.type == 'data':
                attachment_name = attachment.name if hasattr(attachment, 'name') else "unnamed_attachment"
                attachment_path = os.path.join(temp_dir, attachment_name)
                attachment.save(path=attachment_path)
                text = extract_text_from_file(attachment_path)
                if text:
                    texts.append((f"attachment: {attachment_name}", text))
                os.remove(attachment_path)
        logger.info(f"Processed .msg file {msg_path} with {len(texts)} parts.")
        return texts
    except Exception as e:
        logger.error(f"MSG processing error: {e}")
        return []

def extract_text_from_zip(zip_path, temp_dir):
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(temp_dir)
        texts = []
        for root, _, files in os.walk(temp_dir):
            for file in files:
                file_path = os.path.join(root, file)
                text = extract_text_from_file(file_path)
                if text:
                    texts.append((f"file: {file}", text))
        logger.info(f"Processed .zip file {zip_path} with {len(texts)} parts.")
        return texts
    except Exception as e:
        logger.error(f"ZIP processing error: {e}")
        return []

# Unified Text Extraction
def extract_text_from_file(file_path, temp_dir=None):
    if file_path.endswith('.pdf'):
        return extract_text_from_pdf(file_path)
    elif file_path.endswith(('.xls', '.xlsx')):
        return extract_text_from_xls(file_path)
    elif file_path.endswith('.docx'):
        return extract_text_from_docx(file_path)
    elif file_path.endswith('.txt'):
        return extract_text_from_txt(file_path)
    elif file_path.endswith('.csv'):
        return extract_text_from_csv(file_path)
    elif file_path.endswith('.doc'):
        return extract_text_from_doc(file_path)
    elif file_path.endswith('.eml'):
        return extract_text_from_eml(file_path)
    elif file_path.endswith('.msg') and temp_dir:
        return process_msg_file(file_path, temp_dir)
    elif file_path.endswith('.zip') and temp_dir:
        return extract_text_from_zip(file_path, temp_dir)
    else:
        logger.error(f"Unsupported file type: {file_path}")
        return None

# Detect Multiple Languages
def detect_languages(text, max_segments=10):
    try:
        segments = text.split('\n\n')
        if not segments:
            return ["unknown"]
        segments_to_analyze = segments[:max_segments]
        lang_counter = Counter()
        for segment in segments_to_analyze:
            if segment.strip():
                try:
                    lang = detect(segment)
                    lang_counter[lang] += 1
                except LangDetectException:
                    continue
        top_languages = [lang for lang, _ in lang_counter.most_common(3)]
        if not top_languages:
            top_languages = ["unknown"]
        logger.info(f"Detected languages: {', '.join(top_languages)}")
        return top_languages
    except Exception as e:
        logger.error(f"Language detection error: {e}")
        return ["unknown"]

# Initialize the Model with Mixed Precision
def initialize_model(model_path=MODEL_PATH):
    logger.info("Initializing Qwen3-14B-Instruct in 4-bit with mixed precision...")
    try:
        quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)
        model = AutoModelForCausalLM.from_pretrained(
            model_path, device_map="auto", quantization_config=quantization_config, trust_remote_code=True,
            attn_implementation="eager", low_cpu_mem_usage=True
        ).to('cuda')
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, padding_side='left')
        if tokenizer.pad_token is None:
            tokenizer.pad_token_id = tokenizer.eos_token_id
        logger.info("Model initialized successfully on GPU with mixed precision.")
        return model, tokenizer
    except Exception as e:
        logger.error(f"Error initializing model: {e}")
        return None, None

# Dynamic Batch Size Based on GPU Memory
def get_dynamic_batch_size():
    try:
        free_memory = torch.cuda.memory_reserved() - torch.cuda.memory_allocated()
        if free_memory > 10e9:  # More than 10GB free
            return 4  # Reduced from 8
        elif free_memory > 5e9:  # More than 5GB free
            return 2  # Reduced from 4
        else:
            return 1  # Reduced from 2
    except Exception:
        return 2  # Default if GPU not available

# Dynamic Chunk Sizing Based on GPU Memory
def get_dynamic_chunk_size():
    try:
        free_memory = torch.cuda.memory_reserved() - torch.cuda.memory_allocated()
        base_chunk_size = 1500  # Reduced from 3000
        if free_memory < 2e9:  # Less than 2GB free
            return max(500, base_chunk_size // 2)
        elif free_memory < 4e9:  # Less than 4GB free
            return base_chunk_size
        return base_chunk_size * 2
    except Exception:
        return 1500  # Default if GPU not available

# Split Text into Chunks with Dynamic Sizing
def split_text_into_chunks(text, tokenizer, overlap=500):
    if not text:
        logger.warning("No text provided for chunking.")
        return []
    max_chunk_size = get_dynamic_chunk_size()
    tokens = tokenizer.encode(text, add_special_tokens=False)
    chunks = []
    start = 0
    while start < len(tokens):
        end = min(start + max_chunk_size, len(tokens))
        chunk_tokens = tokens[start:end]
        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)
        if chunk_text.strip():
            chunks.append(chunk_text)
        start += max_chunk_size - overlap
        gc.collect()
        torch.cuda.empty_cache()
    logger.info(f"Split into {len(chunks)} chunks with dynamic size {max_chunk_size}.")
    return chunks

# Extract Sales Information from a Batch of Chunks
def extract_sales_info_from_batch(batch_chunks, model, tokenizer, languages, file_name):
    if not batch_chunks:
        return ["No|None|No related information found."] * len(batch_chunks)
    lang_str = ", ".join(languages) if languages else "unknown language(s)"
    batch_messages = [
        [
            {"role": "system", "content": "You are an expert in analyzing financial documents for sales of collateral during restructuring or default processes. You MUST output only the specified format without any additional text, tags (e.g., <think>), or the prompt itself. Any deviation will result in an invalid response."},
            {"role": "user", "content": (
                f"Analyze the following text chunk from a financial document, which may contain multiple languages ({lang_str}). Determine if it mentions a sale of collateral during a default or restructuring process. "
                f"If it does, extract the date of the sale in YYYY-MM-DD format. Provide a short description of the chunk's content and the language(s). "
                f"Output **strictly** in this format: sale_of_collateral|date|comment\n"
                f"Do **not** include any additional text, tags, or the prompt itself. Examples of correct output:\n"
                f"- Yes|2023-01-15|The chunk mentions a sale of collateral on January 15, 2023, during restructuring. Language: English.\n"
                f"- No|None|No mention of collateral sales. The chunk discusses loan terms. Language: Dutch.\n"
                f"Text:\n{chunk}"
            )}
        ] for chunk in batch_chunks
    ]
    try:
        batch_prompts = [tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False) for messages in batch_messages]
        inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=2048)  # Removed invalid flags
        results = [tokenizer.decode(output, skip_special_tokens=True).strip() for output in outputs]
        chunk_file_name = os.path.splitext(os.path.basename(file_name))[0] + "_chunks.txt"
        chunk_file_path = os.path.join(CHUNK_OUTPUTS_DIR, chunk_file_name)
        with open(chunk_file_path, 'a', encoding='utf-8') as f:
            for i, result in enumerate(results):
                f.write(f"Chunk {i+1} Output:\n{result}\n\n")
        logger.info(f"Saved chunk outputs to {chunk_file_path}")
        return results
    except Exception as e:
        logger.error(f"Batch sales info extraction error: {e}")
        return ["Error: LLM generation failed."] * len(batch_chunks)

# Consolidate Chunk Outputs
def consolidate_chunk_outputs(chunk_outputs, model, tokenizer, languages, file_name):
    if not chunk_outputs:
        return "No|None|No sales information found in the document."
    lang_str = ", ".join(languages) if languages else "unknown language(s)"
    consolidated_prompt = (
        f"The following are outputs from analyzing different chunks of a financial document in {lang_str}:\n\n"
    )
    for i, output in enumerate(chunk_outputs, 1):
        consolidated_prompt += f"Chunk {i} Output:\n{output}\n\n"
    consolidated_prompt += (
        f"Based on these outputs, provide a consolidated summary for the entire document. Determine if there was a sale of collateral during a default or restructuring process. "
        f"If yes, provide the most relevant sale date in YYYY-MM-DD format and a brief comment summarizing the document content and languages. "
        f"If no, state 'No' and explain why. "
        f"Output **strictly** in this format: sale_of_collateral|date|comment\n"
        f"Do **not** include any additional text, tags, or the prompt itself. Examples of correct output:\n"
        f"- Yes|2023-01-15|The document discusses a sale of collateral on January 15, 2023, during restructuring. Languages: English, French.\n"
        f"- No|None|No evidence of collateral sales across all chunks. The document discusses loan terms. Languages: Dutch, English.\n"
        f"Ensure the comment includes a short description of the document and mentions the languages ({lang_str})."
    )
    messages = [
        {"role": "system", "content": "You are an expert in consolidating financial document analyses. You MUST output only the specified format without any additional text, tags, or the prompt itself."},
        {"role": "user", "content": consolidated_prompt}
    ]
    try:
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(model.device)
        with torch.no_grad():
            out_ids = model.generate(**inputs, max_new_tokens=2048)  # Removed invalid flags
            result = tokenizer.decode(out_ids[0], skip_special_tokens=True).strip()
        chunk_file_name = os.path.splitext(os.path.basename(file_name))[0] + "_consolidated.txt"
        chunk_file_path = os.path.join(CHUNK_OUTPUTS_DIR, chunk_file_name)
        with open(chunk_file_path, 'w', encoding='utf-8') as f:
            f.write(f"Consolidated Output:\n{result}\n")
        logger.info(f"Saved consolidated output to {chunk_file_path}")
        return result
    except Exception as e:
        logger.error(f"Consolidation error: {e}")
        return "Error: Consolidation failed."

# Parse Sales Output with Regex
def process_sales_output(raw_output):
    if not raw_output or "Error:" in raw_output:
        return {
            "sale_of_collateral": "No",
            "date": None,
            "comment": f"Model output error: {raw_output}",
            "raw_response": raw_output
        }
    try:
        pattern = r"(Yes|No)\|(\d{4}-\d{2}-\d{2}|None)\|(.*?)(?:\n|$)"
        matches = re.findall(pattern, raw_output, re.DOTALL)
        if matches:
            sale_of_collateral, date_str, comment = matches[-1]
            date = date_str if sale_of_collateral == "Yes" and date_str != "None" else None
            return {
                "sale_of_collateral": sale_of_collateral,
                "date": date,
                "comment": comment.strip(),
                "raw_response": f"{sale_of_collateral}|{date_str}|{comment.strip()}"
            }
        else:
            logger.warning(f"Failed to parse model output: {raw_output}")
            return {
                "sale_of_collateral": "No",
                "date": None,
                "comment": "Failed to parse model output.",
                "raw_response": raw_output
            }
    except Exception as e:
        logger.error(f"Error parsing sales output: {e}")
        return {
            "sale_of_collateral": "No",
            "date": None,
            "comment": f"Parsing error: {str(e)}",
            "raw_response": raw_output
        }

# Standardize Date Format
def standardize_date(date_str):
    try:
        date_obj = date_parser.parse(date_str, dayfirst=True)
        return date_obj.strftime("%Y-%m-%d")
    except Exception as e:
        logger.error(f"Date standardization error: {e}")
        return date_str

# Analyze Document with Batch Processing
def analyze_document(text, model, tokenizer, languages, file_name):
    if not text:
        return {"sale_of_collateral": "No", "date": None, "comment": "No text provided.", "raw_response": ""}
    chunks = split_text_into_chunks(text, tokenizer)
    chunk_outputs = []
    batch_size = get_dynamic_batch_size()
    for i in range(0, len(chunks), batch_size):
        batch_chunks = chunks[i:i + batch_size]
        try:
            batch_outputs = extract_sales_info_from_batch(batch_chunks, model, tokenizer, languages, file_name)
            chunk_outputs.extend(batch_outputs)
        except Exception as e:
            logger.error(f"Error processing batch {i//batch_size + 1}: {e}")
            chunk_outputs.extend(["Error: Batch processing failed."] * len(batch_chunks))
        gc.collect()
        torch.cuda.empty_cache()
    consolidated_output = consolidate_chunk_outputs(chunk_outputs, model, tokenizer, languages, file_name)
    sales_info = process_sales_output(consolidated_output)
    return sales_info

# Extract Archive
def extract_archive(archive_path, extract_to):
    if archive_path.endswith('.zip'):
        with zipfile.ZipFile(archive_path, 'r') as zip_ref:
            zip_ref.extractall(extract_to)
    elif archive_path.endswith('.7z'):
        with py7zr.SevenZipFile(archive_path, 'r') as seven_zip:
            seven_zip.extractall(extract_to)
    else:
        raise ValueError("Unsupported archive type. Only .zip and .7z are supported.")

# Process a Single File with Intermediate Saving
def process_file(file_path, company_name, model, tokenizer, temp_dir, include_raw_response=True):
    try:
        result = extract_text_from_file(file_path, temp_dir)
        if isinstance(result, list):  # For .msg and .zip files
            results = []
            for part, text in result:
                if text:
                    languages = detect_languages(text)
                    sales_info = analyze_document(text, model, tokenizer, languages, file_path)
                    df_row = {
                        "company": company_name,
                        "file_name": os.path.basename(file_path),
                        "part": part,
                        "sale_of_collateral": sales_info["sale_of_collateral"],
                        "date": sales_info["date"],
                        "comment": sales_info["comment"],
                        "languages": ", ".join(languages)
                    }
                    if include_raw_response:
                        df_row["raw_response"] = sales_info.get("raw_response", "")
                    results.append(df_row)
            df = pd.DataFrame(results)
        else:  # For single-text files
            text = result
            if not text:
                logger.error(f"No text extracted from {file_path}. Skipping.")
                return pd.DataFrame()
            languages = detect_languages(text)
            sales_info = analyze_document(text, model, tokenizer, languages, file_path)
            df_row = {
                "company": company_name,
                "file_name": os.path.basename(file_path),
                "part": "main",
                "sale_of_collateral": sales_info["sale_of_collateral"],
                "date": sales_info["date"],
                "comment": sales_info["comment"],
                "languages": ", ".join(languages)
            }
            if include_raw_response:
                df_row["raw_response"] = sales_info.get("raw_response", "")
            df = pd.DataFrame([df_row])
        
        if not df.empty:
            df.to_csv(INTERMEDIATE_CSV, mode='a', header=not os.path.exists(INTERMEDIATE_CSV), index=False)
            logger.info(f"Processed {file_path}. Saved intermediate result.")
        return df
    except Exception as e:
        logger.exception(f"Error processing file {file_path}: {e}")
        return pd.DataFrame()

# Process a Single Company Folder
def process_company_folder(company_folder, model, tokenizer, temp_dir, include_raw_response=True):
    company_path = os.path.join(temp_dir, company_folder)
    if not os.path.isdir(company_path):
        logger.error(f"Company folder {company_folder} does not exist in {temp_dir}.")
        return pd.DataFrame()
    company_name = company_folder
    results = []
    for file in os.listdir(company_path):
        file_path = os.path.join(company_path, file)
        df = process_file(file_path, company_name, model, tokenizer, temp_dir, include_raw_response)
        if not df.empty:
            results.append(df)
    if results:
        combined = pd.concat(results, ignore_index=True)
        print(f"\n\n=== Results for {company_name} ===")
        print(combined.to_string(index=False))
        logger.info(f"Processed {len(results)} documents for {company_name} with {len(combined)} total entries.")
        return combined
    else:
        print(f"No sales information found for {company_name}.")
        logger.info(f"No sales information found in any processed file for {company_name}.")
        return pd.DataFrame()

# Main Execution
if __name__ == "__main__":
    import sys
    import argparse

    # Remove the '-f' argument if itâ€™s present (Jupyter-specific fix)
    if '-f' in sys.argv:
        sys.argv.remove('-f')
    
    parser = argparse.ArgumentParser(description="Process sales information from financial documents.")
    parser.add_argument("company", type=str, nargs='?', default=None, help="Specify a single company folder to process for testing.")
    args = parser.parse_args()
    
    model, tokenizer = initialize_model()
    if model is None or tokenizer is None:
        logger.error("Model initialization failed. Exiting.")
    else:
        temp_dir = tempfile.mkdtemp()
        try:
            extract_archive(ARCHIVE_PATH, temp_dir)
            if args.company:
                # Process only the specified company folder
                combined = process_company_folder(args.company, model, tokenizer, temp_dir, include_raw_response=True)
                if not combined.empty:
                    combined.to_csv(OUTPUT_CSV, index=False)
            else:
                # Process all company folders
                results = []
                for company_folder in os.listdir(temp_dir):
                    company_path = os.path.join(temp_dir, company_folder)
                    if os.path.isdir(company_path):
                        company_name = company_folder
                        for file in os.listdir(company_path):
                            file_path = os.path.join(company_path, file)
                            df = process_file(file_path, company_name, model, tokenizer, temp_dir, include_raw_response=True)
                            if not df.empty:
                                results.append(df)
                if results:
                    combined = pd.concat(results, ignore_index=True)
                    print("\n\n=== Combined Results From All Files ===")
                    print(combined.to_string(index=False))
                    logger.info(f"Processed {len(results)} documents with {len(combined)} total entries.")
                    combined.to_csv(OUTPUT_CSV, index=False)
                else:
                    print("No sales information found.")
                    logger.info("No sales information found in any processed file.")
        except Exception as e:
            logger.error(f"Error processing archive: {e}")
        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)
