import sys
sys.argv = ['script.py', '5 - Pret 44A Holding B.V'] # For testing a specific company
import os
import zipfile
import py7zr
import shutil
import tempfile
import torch
import pandas as pd
import logging
import re
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import pymupdf as fitz # PyMuPDF
import easyocr
from langdetect import detect, LangDetectException
from dateutil import parser as date_parser
from collections import Counter
import pkg_resources
import extract_msg
from docx import Document
import textract
import email
import gc
import argparse

# Logging Setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
logger = logging.getLogger(__name__)

# Configuration
MODEL_PATH = "/home/inghero/data/irwbds/llm/parc/qwen3_14B"
ARCHIVE_PATH = "/home/inghero/data/irwbds/llm/parc/notebooks/LGD/input/sales_conf.7z"
OUTPUT_CSV = "/home/inghero/data/irwbds/llm/parc/notebooks/LGD/output/sales_info_7.csv"
INTERMEDIATE_CSV = "/home/inghero/data/irwbds/llm/parc/notebooks/LGD/output/intermediate_results_6.csv"
EXTRACTED_TEXT_DIR = "extracted_texts_7"
CHUNK_OUTPUTS_DIR = "chunk_outputs_7"
DPI = 300  # MODIFIED: 400 is very high and memory-intensive; 300 is usually sufficient.

# Ensure directories exist
for directory in [EXTRACTED_TEXT_DIR, CHUNK_OUTPUTS_DIR]:
    os.makedirs(directory, exist_ok=True)

def check_pymupdf_version():
    try:
        pymupdf_version = pkg_resources.get_distribution("pymupdf").version
        logger.info(f"PyMuPDF version: {pymupdf_version}")
        return pymupdf_version
    except Exception as e:
        logger.error(f"Error checking PyMuPDF version: {e}")
        return None

def verify_easyocr_models(model_dir="~/.EasyOCR/model"):
    model_dir = os.path.expanduser(model_dir)
    required_models = ['craft_mlt_25k.pth', 'latin_g2.pth']
    missing_models = [model for model in required_models if not os.path.exists(os.path.join(model_dir, model))]
    if missing_models:
        logger.error(f"Missing EasyOCR models: {', '.join(missing_models)} in {model_dir}")
        return False
    logger.info(f"All required EasyOCR models found in {model_dir}")
    return True

def ocr_page(page, reader, dpi):
    try:
        pix = page.get_pixmap(dpi=dpi)
        img_path = f"temp_page_{page.number}.png"
        pix.save(img_path)
        result = reader.readtext(img_path)
        text = "\n".join([res[1] for res in result])
        os.remove(img_path)
        return text
    except Exception as e:
        logger.error(f"OCR error for page {page.number}: {e}")
        return ""

def process_page(page, reader, dpi, text_threshold=50):
    try:
        if page.rotation != 0:
            page.set_rotation(0)
        txt = page.get_text("text")
        if len(txt.strip()) >= text_threshold:
            logger.info(f"Page {page.number}: Using direct text extraction.")
            return txt
        else:
            logger.info(f"Page {page.number}: Using OCR.")
            return ocr_page(page, reader, dpi)
    except Exception as e:
        logger.error(f"Error processing page {page.number}: {e}")
        return ""

def extract_text_from_pdf(pdf_path, languages=['en', 'nl', 'fr'], dpi=DPI, text_threshold=100):
    full_text = ""
    reader = None
    try:
        check_pymupdf_version()
        doc = fitz.open(pdf_path)
        use_ocr = doc.is_form_pdf or any(len(page.get_text("text").strip()) < text_threshold for page in doc)
        
        if use_ocr:
            if verify_easyocr_models():
                logger.info("Initializing EasyOCR for PDF processing...")
                reader = easyocr.Reader(languages) # Initialize only when needed
            else:
                logger.warning(f"OCR needed but EasyOCR models missing for {pdf_path}.")

        for page in doc:
            if reader:
                text = process_page(page, reader, dpi, text_threshold)
            else:
                text = page.get_text("text")
            full_text += text + "\n\n"
        
        doc.close()
        logger.info(f"Extracted text from {pdf_path} ({len(full_text.split())} words).")
        text_file_name = os.path.splitext(os.path.basename(pdf_path))[0] + ".txt"
        text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
        with open(text_file_path, 'w', encoding='utf-8') as f:
            f.write(full_text)
        logger.info(f"Saved extracted text to {text_file_path}")
    except Exception as e:
        logger.error(f"Error processing {pdf_path}: {e}")
        return None
    finally:
        if reader:
            del reader
            gc.collect()
            torch.cuda.empty_cache()
    return full_text if full_text else None
# ...[ALL YOUR OTHER extract_text_from_... FUNCTIONS GO HERE]...
# (The same applies to your other text extraction functions, keep them as they are)
def extract_text_from_xls(xls_path):
    try:
        df = pd.read_excel(xls_path, sheet_name=None)
        full_text = ""
        for sheet_name, sheet_df in df.items():
            full_text += sheet_df.to_string() + "\n\n"
        logger.info(f"Extracted text from {xls_path} ({len(full_text.split())} words).")
        text_file_name = os.path.splitext(os.path.basename(xls_path))[0] + ".txt"
        text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
        with open(text_file_path, 'w', encoding='utf-8') as f:
            f.write(full_text)
        logger.info(f"Saved extracted text to {text_file_path}")
        return full_text
    except Exception as e:
        logger.error(f"XLS extraction error: {e}")
        return None

def extract_text_from_docx(docx_path):
    try:
        doc = Document(docx_path)
        text = "\n".join([para.text for para in doc.paragraphs])
        logger.info(f"Extracted text from {docx_path} ({len(text.split())} words).")
        text_file_name = os.path.splitext(os.path.basename(docx_path))[0] + ".txt"
        text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
        with open(text_file_path, 'w', encoding='utf-8') as f:
            f.write(text)
        logger.info(f"Saved extracted text to {text_file_path}")
        return text
    except Exception as e:
        logger.error(f"DOCX extraction error: {e}")
        return None

def extract_text_from_txt(txt_path):
    try:
        with open(txt_path, 'r', encoding='utf-8') as f:
            text = f.read()
        logger.info(f"Extracted text from {txt_path} ({len(text.split())} words).")
        text_file_name = os.path.splitext(os.path.basename(txt_path))[0] + ".txt"
        text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
        with open(text_file_path, 'w', encoding='utf-8') as f:
            f.write(text)
        logger.info(f"Saved extracted text to {text_file_path}")
        return text
    except Exception as e:
        logger.error(f"TXT extraction error: {e}")
        return None

def extract_text_from_csv(csv_path):
    try:
        df = pd.read_csv(csv_path)
        text = df.to_string(index=False)
        logger.info(f"Extracted text from {csv_path} ({len(text.split())} words).")
        text_file_name = os.path.splitext(os.path.basename(csv_path))[0] + ".txt"
        text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
        with open(text_file_path, 'w', encoding='utf-8') as f:
            f.write(text)
        logger.info(f"Saved extracted text to {text_file_path}")
        return text
    except Exception as e:
        logger.error(f"CSV extraction error: {e}")
        return None

def extract_text_from_doc(doc_path):
    try:
        text = textract.process(doc_path, method='libreoffice').decode('utf-8')
        logger.info(f"Extracted text from {doc_path} ({len(text.split())} words).")
        text_file_name = os.path.splitext(os.path.basename(doc_path))[0] + ".txt"
        text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
        with open(text_file_path, 'w', encoding='utf-8') as f:
            f.write(text)
        logger.info(f"Saved extracted text to {text_file_path}")
        return text
    except Exception as e:
        logger.error(f"DOC extraction error: {e}. Ensure LibreOffice is installed (e.g., 'sudo apt-get install libreoffice' on Ubuntu).")
        return None

def extract_text_from_eml(eml_path):
    try:
        with open(eml_path, 'r', encoding='utf-8') as f:
            msg = email.message_from_file(f)
        text = ""
        if msg.is_multipart():
            for part in msg.walk():
                content_type = part.get_content_type()
                if content_type == 'text/plain':
                    text += part.get_payload(decode=True).decode('utf-8', errors='ignore')
                elif content_type == 'text/html':
                    text += part.get_payload(decode=True).decode('utf-8', errors='ignore')
        else:
            text = msg.get_payload(decode=True).decode('utf-8', errors='ignore')
        logger.info(f"Extracted text from {eml_path} ({len(text.split())} words).")
        text_file_name = os.path.splitext(os.path.basename(eml_path))[0] + ".txt"
        text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
        with open(text_file_path, 'w', encoding='utf-8') as f:
            f.write(text)
        logger.info(f"Saved extracted text to {text_file_path}")
        return text
    except Exception as e:
        logger.error(f"EML extraction error: {e}")
        return None

def process_msg_file(msg_path, temp_dir):
    try:
        msg = extract_msg.Message(msg_path)
        texts = []
        if msg.body:
            texts.append(("body", msg.body))
            text_file_name = os.path.splitext(os.path.basename(msg_path))[0] + "_body.txt"
            text_file_path = os.path.join(EXTRACTED_TEXT_DIR, text_file_name)
            with open(text_file_path, 'w', encoding='utf-8') as f:
                f.write(msg.body)
            logger.info(f"Saved extracted text to {text_file_path}")
        for attachment in msg.attachments:
            if attachment.type == 'data':
                attachment_name = attachment.name if hasattr(attachment, 'name') else "unnamed_attachment"
                attachment_path = os.path.join(temp_dir, attachment_name)
                attachment.save(path=attachment_path)
                text = extract_text_from_file(attachment_path)
                if text:
                    texts.append((f"attachment: {attachment_name}", text))
                os.remove(attachment_path)
        logger.info(f"Processed .msg file {msg_path} with {len(texts)} parts.")
        return texts
    except Exception as e:
        logger.error(f"MSG processing error: {e}")
        return []

# Replace your existing extract_text_from_zip function with this one.

def extract_text_from_zip(zip_path, temp_dir):
    # Create a UNIQUE, dedicated sub-directory for this specific zip's contents.
    # This prevents it from scanning other company folders.
    zip_extract_dir = os.path.join(temp_dir, os.path.basename(zip_path) + "_contents")
    os.makedirs(zip_extract_dir, exist_ok=True)
    
    logger.info(f"Extracting '{os.path.basename(zip_path)}' to its own temp folder: {zip_extract_dir}")

    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(zip_extract_dir) # Extract to the DEDICATED directory
        
        texts = []
        # CRUCIAL CHANGE: Walk only within the dedicated extraction directory
        for root, _, files in os.walk(zip_extract_dir):
            for file in files:
                file_path_in_zip = os.path.join(root, file)
                
                # We need to pass the main temp_dir to the recursive call if it encounters another archive
                # This is a safe way to handle nested archives if they exist.
                text_content = extract_text_from_file(file_path_in_zip, temp_dir) 

                if isinstance(text_content, list):
                    # If we found a nested zip/msg, add all its parts
                    logger.info(f"  -> Found nested archive '{file}', adding its {len(text_content)} parts.")
                    texts.extend(text_content)
                elif text_content and text_content.strip():
                    # It's a regular file with text
                    texts.append((f"file_in_zip: {file}", text_content))
                else:
                    logger.warning(f"  -> No text extracted from '{file}' inside the zip. Skipping.")

        logger.info(f"Finished processing .zip file {os.path.basename(zip_path)}. Found {len(texts)} text parts.")
        return texts
    except Exception as e:
        logger.error(f"ZIP processing error for {zip_path}: {e}")
        return []
    finally:
        # Clean up the dedicated directory for this zip file after we are done with it.
        shutil.rmtree(zip_extract_dir, ignore_errors=True)
        
# Unified Text Extraction
def extract_text_from_file(file_path, temp_dir=None):
    if file_path.endswith('.pdf'):
        return extract_text_from_pdf(file_path)
    elif file_path.endswith(('.xls', '.xlsx')):
        return extract_text_from_xls(file_path)
    elif file_path.endswith('.docx'):
        return extract_text_from_docx(file_path)
    elif file_path.endswith('.txt'):
        return extract_text_from_txt(file_path)
    elif file_path.endswith('.csv'):
        return extract_text_from_csv(file_path)
    elif file_path.endswith('.doc'):
        return extract_text_from_doc(file_path)
    elif file_path.endswith('.eml'):
        return extract_text_from_eml(file_path)
    elif file_path.endswith('.msg') and temp_dir:
        return process_msg_file(file_path, temp_dir)
    elif file_path.endswith('.zip') and temp_dir:
        return extract_text_from_zip(file_path, temp_dir)
    else:
        logger.warning(f"Unsupported file type: {file_path}, skipping.")
        return None

def detect_languages(text, max_segments=10):
    try:
        segments = text.split('\n\n')
        if not segments: return ["unknown"]
        segments_to_analyze = [s for s in segments if s.strip()][:max_segments]
        if not segments_to_analyze: return ["unknown"]
        lang_counter = Counter(detect(s) for s in segments_to_analyze)
        top_languages = [lang for lang, _ in lang_counter.most_common(3)]
        if not top_languages: top_languages = ["unknown"]
        logger.info(f"Detected languages: {', '.join(top_languages)}")
        return top_languages
    except Exception as e:
        logger.error(f"Language detection error: {e}")
        return ["unknown"]

# MODIFIED: Model initialization with flash_attention_2
def initialize_model(model_path=MODEL_PATH):
    logger.info("Initializing Qwen3-14B-Instruct in 4-bit with mixed precision...")
    try:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            quantization_config=quantization_config,
            trust_remote_code=True,
            attn_implementation="sdpa",  # MODIFIED: Highly recommended for A100/H100
            low_cpu_mem_usage=True
        )
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, padding_side='left')
        if tokenizer.pad_token is None:
            tokenizer.pad_token_id = tokenizer.eos_token_id
        logger.info("Model initialized successfully on GPU with flash_attention_2.")
        return model, tokenizer
    except Exception as e:
        logger.error(f"Error initializing model: {e}. If 'flash_attention_2' fails, try 'eager' and ensure flash-attn is installed (`pip install flash-attn --no-build-isolation`).")
        return None, None

# MODIFIED: Use torch.cuda.mem_get_info for more accurate memory readings
def get_dynamic_batch_size():
    if not torch.cuda.is_available(): return 1
    free_memory, _ = torch.cuda.mem_get_info()
    free_gb = free_memory / (1024**3)
    if free_gb > 15: return 4
    if free_gb > 8: return 2
    return 1

def get_dynamic_chunk_size():
    if not torch.cuda.is_available(): return 1500
    free_memory, _ = torch.cuda.mem_get_info()
    free_gb = free_memory / (1024**3)
    if free_gb < 4: return 750
    if free_gb < 8: return 1500
    return 3000

def split_text_into_chunks(text, tokenizer, overlap=500):
    if not text:
        logger.warning("No text provided for chunking.")
        return []
    max_chunk_size = get_dynamic_chunk_size()
    tokens = tokenizer.encode(text, add_special_tokens=False)
    chunks = []
    start = 0
    while start < len(tokens):
        end = min(start + max_chunk_size, len(tokens))
        chunk_text = tokenizer.decode(tokens[start:end], skip_special_tokens=True)
        if chunk_text.strip():
            chunks.append(chunk_text)
        start += max_chunk_size - overlap
    logger.info(f"Split into {len(chunks)} chunks with dynamic size {max_chunk_size}.")
    return chunks

# MODIFIED: Now just returns results, file saving is handled by the caller
def extract_sales_info_from_batch(batch_chunks, model, tokenizer, languages):
    if not batch_chunks: return []
    lang_str = ", ".join(languages) if languages else "unknown"
    batch_messages = [
        [
            {"role": "system", "content": "You are a financial document analysis expert. Your task is to analyze text chunks for mentions of collateral sales in default/restructuring contexts. Output ONLY in the format: sale_of_collateral|date|comment. No extra text or tags."},
            {"role": "user", "content": (
                f"Analyze this text chunk from a document in {lang_str}. Does it mention a sale of ccollateral/property during default/restructuring or during company issues? "
                f"If yes, extract the sale date (YYYY-MM-DD). Provide a compact but reasonable content description. "
                f"Strictly format your output as: sale_of_collateral|date|comment\n"
                f"Example: Yes|2023-01-15|Chunk mentions collateral sale on Jan 15, 2023. Language: English.\n"
                f"Example: No|None|No mention of collateral sales; discusses loan terms. Language: Dutch.\n"
                f"Text:\n{chunk}"
            )}
        ] for chunk in batch_chunks
    ]
    try:
        batch_prompts = [tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True,  enable_thinking=False) for messages in batch_messages]
        inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=150, do_sample=False) # MODIFIED: Reduced max_new_tokens
        results = [tokenizer.decode(output[inputs.input_ids.shape[1]:], skip_special_tokens=True).strip() for output in outputs]
        
        # NEW: Explicitly delete tensors to help memory management
        del inputs, outputs, batch_prompts
        return results
    except Exception as e:
        logger.error(f"Batch sales info extraction error: {e}")
        return [f"Error:LLM generation failed|None|{e}"] * len(batch_chunks)

# MODIFIED: Consolidation prompt logic remains, but is now called by the iterative function
def consolidate_chunk_outputs(chunk_outputs, model, tokenizer, languages, file_name):
    if not chunk_outputs: return "No|None|No sales information found."
    lang_str = ", ".join(languages) if languages else "unknown"
    
    # Create the prompt from the provided chunks/summaries
    consolidated_input = "\n\n".join(f"Analysis of Chunk/Summary {i+1}:\n{output}" for i, output in enumerate(chunk_outputs))
    
    messages = [
        {"role": "system", "content": "You are an expert consolidator of financial analyses. You MUST output only the specified format without any additional text or tags."},
        {"role": "user", "content": (
            f"Below are several analyses from a single financial document in {lang_str}. Consolidate them into a final answer. "
            f"Was there a sale of collateral/property during default/restructuring or during company issues? If yes, find the most relevant date (YYYY-MM-DD). "
            f"Summarize the document's purpose and languages. "
            f"Strictly format your output as: sale_of_collateral|date|comment\n"
            f"Example: Yes|2023-01-15|Document confirms collateral/property sale on Jan 15, 2023, during restructuring or company issues. Languages: English, French.\n"
            f"Example: No|None|No evidence of collateral sales found. Document discusses loan terms. Languages: Dutch.\n\n"
            f"Analyses to consolidate:\n{consolidated_input}"
        )}
    ]
    try:
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True,  enable_thinking=False)
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=model.config.max_position_embeddings - 256).to(model.device)
        with torch.no_grad():
            out_ids = model.generate(**inputs, max_new_tokens=256, do_sample=False)
            result = tokenizer.decode(out_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()

        # Save the output for this specific consolidation step for debugging
        chunk_file_name = os.path.splitext(os.path.basename(file_name))[0] + "_consolidated_step.txt"
        chunk_file_path = os.path.join(CHUNK_OUTPUTS_DIR, chunk_file_name)
        with open(chunk_file_path, 'w', encoding='utf-8') as f:
            f.write(f"Consolidated Output Step:\n{result}\n")
        logger.info(f"Saved consolidation step output to {chunk_file_path}")

        del inputs, out_ids, prompt
        return result
    except Exception as e:
        logger.error(f"Consolidation error: {e}")
        return f"Error:Consolidation failed|None|{e}"

# NEW: The robust, iterative consolidation function to avoid huge prompts
def iterative_consolidate_chunk_outputs(chunk_outputs, model, tokenizer, languages, file_name, group_size=5):
    if not chunk_outputs:
        return "No|None|No sales information found as no chunks were provided."
    if len(chunk_outputs) <= group_size:
        logger.info(f"Consolidating {len(chunk_outputs)} chunks in a single pass.")
        return consolidate_chunk_outputs(chunk_outputs, model, tokenizer, languages, file_name)

    logger.info(f"Starting iterative consolidation for {len(chunk_outputs)} chunks with group size {group_size}.")
    current_summaries = list(chunk_outputs)
    level = 1
    while len(current_summaries) > 1:
        logger.info(f"Consolidation Level {level}: Processing {len(current_summaries)} summaries into smaller groups.")
        next_level_summaries = []
        for i in range(0, len(current_summaries), group_size):
            group = current_summaries[i:i + group_size]
            if len(group) == 1:
                next_level_summaries.append(group[0])
                continue
            
            logger.info(f"  - Consolidating group of {len(group)} summaries.")
            step_file_name = f"{os.path.splitext(file_name)[0]}_level{level}_group{i//group_size}"
            consolidated_summary = consolidate_chunk_outputs(group, model, tokenizer, languages, step_file_name)
            
            # Simple check for error format
            if consolidated_summary.lower().startswith("error:"):
                 logger.error(f"    - Error during iterative consolidation: {consolidated_summary}. Skipping this group's content for final result.")
            else:
                next_level_summaries.append(consolidated_summary)
            
            gc.collect()
            torch.cuda.empty_cache()
        
        current_summaries = next_level_summaries
        level += 1
    
    logger.info("Iterative consolidation complete.")
    return current_summaries[0] if current_summaries else "Error:Consolidation failed|None|No summaries produced."

# MODIFIED: analyze_document now calls the iterative consolidator
def analyze_document(text, model, tokenizer, languages, file_name):
    if not text:
        return {"sale_of_collateral": "No", "date": None, "comment": "No text provided.", "raw_response": ""}
    
    chunks = split_text_into_chunks(text, tokenizer)
    if not chunks:
        return {"sale_of_collateral": "No", "date": None, "comment": "Text was too short to create chunks.", "raw_response": ""}

    chunk_outputs = []
    batch_size = get_dynamic_batch_size()
    
    for i in range(0, len(chunks), batch_size):
        batch_chunks = chunks[i:i + batch_size]
        try:
            batch_outputs = extract_sales_info_from_batch(batch_chunks, model, tokenizer, languages)
            chunk_outputs.extend(batch_outputs)
        except Exception as e:
            logger.error(f"Error processing batch {i//batch_size + 1}: {e}")
            chunk_outputs.extend([f"Error:Batch processing failed|None|{e}"] * len(batch_chunks))
        
        gc.collect()
        torch.cuda.empty_cache()

    # MODIFIED: Efficiently save all chunk outputs at once
    chunk_file_name = os.path.splitext(os.path.basename(file_name))[0] + "_chunks_all.txt"
    chunk_file_path = os.path.join(CHUNK_OUTPUTS_DIR, chunk_file_name)
    with open(chunk_file_path, 'w', encoding='utf-8') as f:
        for i, result in enumerate(chunk_outputs):
            f.write(f"--- Chunk {i+1} Output ---\n{result}\n\n")
    logger.info(f"Saved all chunk outputs to {chunk_file_path}")

    # MODIFIED: Call the new iterative consolidation function
    consolidated_output = iterative_consolidate_chunk_outputs(chunk_outputs, model, tokenizer, languages, file_name)
    sales_info = process_sales_output(consolidated_output)
    return sales_info

# Unchanged functions: process_sales_output, standardize_date, extract_archive, process_file, process_company_folder, main
# These functions seem correct and their logic doesn't need to be changed to fix the core error.
# Please ensure you copy them from your original script.
# ... [Your existing process_sales_output, standardize_date, etc., functions go here] ...
def process_sales_output(raw_output):
    if not raw_output or raw_output.lower().startswith("error:"):
        parts = raw_output.split('|', 2)
        return {
            "sale_of_collateral": "Error", "date": None,
            "comment": parts[2] if len(parts) > 2 else raw_output,
            "raw_response": raw_output
        }
    try:
        # Be more lenient with parsing
        parts = [p.strip() for p in raw_output.split('|')]
        if len(parts) >= 3:
            sale_of_collateral = "Yes" if "yes" in parts[0].lower() else "No"
            date_str = parts[1] if parts[1].lower() != "none" else None
            comment = "|".join(parts[2:])
            return {
                "sale_of_collateral": sale_of_collateral,
                "date": date_str, "comment": comment, "raw_response": raw_output
            }
        else:
            logger.warning(f"Failed to parse model output: {raw_output}")
            return {"sale_of_collateral": "Parse Error", "date": None, "comment": "Failed to parse model output.", "raw_response": raw_output}
    except Exception as e:
        logger.error(f"Error parsing sales output: {e}")
        return {"sale_of_collateral": "Parse Error", "date": None, "comment": f"Parsing error: {e}", "raw_response": raw_output}
        
def extract_archive(archive_path, extract_to):
    if archive_path.endswith('.zip'):
        with zipfile.ZipFile(archive_path, 'r') as zip_ref:
            zip_ref.extractall(extract_to)
    elif archive_path.endswith('.7z'):
        with py7zr.SevenZipFile(archive_path, 'r') as seven_zip:
            seven_zip.extractall(extract_to)
    else:
        raise ValueError("Unsupported archive type. Only .zip and .7z are supported.")

def process_file(file_path, company_name, model, tokenizer, temp_dir, include_raw_response=True):
    """
    Processes a single file: extracts text, analyzes it with the LLM, and saves the results.
    This version includes enhanced step-by-step logging to diagnose crashes.
    """
    logger.info(f"--------------------------------------------------")
    logger.info(f"STARTING PROCESS_FILE FOR: {os.path.basename(file_path)}")
    try:
        # STAGE 1: Text Extraction
        logger.info("  Step 1: Starting text extraction...")
        result = extract_text_from_file(file_path, temp_dir)
        logger.info("  Step 1: Text extraction finished.")

        all_dfs = []

        # This block handles files that return multiple text parts, like .msg or .zip
        if isinstance(result, list):
            logger.info(f"  -> Found {len(result)} parts to process (from MSG/ZIP).")
            for i, (part, text) in enumerate(result, 1):
                logger.info(f"    -> Processing Part {i}/{len(result)}: '{part}'")
                if text and text.strip():
                    # STAGE 2: Language Detection (per part)
                    logger.info(f"      -> Step 2: Starting language detection for part '{part}'...")
                    languages = detect_languages(text)
                    logger.info(f"      -> Step 2: Language detection finished.")

                    # STAGE 3: LLM Analysis (per part)
                    logger.info(f"      -> Step 3: Starting LLM analysis for part '{part}'...")
                    sales_info = analyze_document(text, model, tokenizer, languages, f"{os.path.basename(file_path)}_{part}")
                    logger.info(f"      -> Step 3: LLM analysis finished.")
                    
                    df_row = {"company": company_name, "file_name": os.path.basename(file_path), "part": part, **sales_info, "languages": ", ".join(languages)}
                    if not include_raw_response and "raw_response" in df_row: del df_row["raw_response"]
                    all_dfs.append(pd.DataFrame([df_row]))
                else:
                    logger.warning(f"    -> Part '{part}' has no text content. Skipping.")
        
        # This block handles files that return a single block of text
        elif result and result.strip():
            text = result
            # STAGE 2: Language Detection
            logger.info("  Step 2: Starting language detection...")
            languages = detect_languages(text)
            logger.info("  Step 2: Language detection finished.")

            # STAGE 3: LLM Analysis
            logger.info("  Step 3: Starting LLM analysis (analyze_document)...")
            sales_info = analyze_document(text, model, tokenizer, languages, file_path)
            logger.info("  Step 3: LLM analysis finished.")
            
            df_row = {"company": company_name, "file_name": os.path.basename(file_path), "part": "main", **sales_info, "languages": ", ".join(languages)}
            if not include_raw_response and "raw_response" in df_row: del df_row["raw_response"]
            all_dfs.append(pd.DataFrame([df_row]))
        
        else:
             logger.error(f"  -> No text was extracted from {os.path.basename(file_path)}. Skipping file.")
             return pd.DataFrame()
        
        # STAGE 4: Aggregation and Saving to Intermediate CSV
        if all_dfs:
            logger.info("  Step 4: Aggregating results and saving to intermediate CSV...")
            final_df = pd.concat(all_dfs, ignore_index=True)
            # Ensure the file exists with a header before appending
            header_needed = not os.path.exists(INTERMEDIATE_CSV)
            final_df.to_csv(INTERMEDIATE_CSV, mode='a', header=header_needed, index=False)
            logger.info(f"  Step 4: Intermediate result saved successfully.")
            logger.info(f"COMPLETED PROCESS_FILE FOR: {os.path.basename(file_path)}")
            return final_df
        else:
            logger.warning(f"  -> No processable data was generated for {os.path.basename(file_path)}. Nothing to save.")
            return pd.DataFrame()

    except Exception as e:
        # This will catch any unexpected crash and log the full error traceback
        logger.exception(f"FATAL unhandled error while processing file {os.path.basename(file_path)}: {e}")
        return pd.DataFrame()

def process_company_folder(company_folder, model, tokenizer, temp_dir, include_raw_response=True):
    company_path = os.path.join(temp_dir, company_folder)
    if not os.path.isdir(company_path):
        logger.error(f"Company folder {company_folder} does not exist in {temp_dir}.")
        return pd.DataFrame()
    company_name = company_folder
    results = []
    for file_name in sorted(os.listdir(company_path)): # Sort for deterministic processing
        file_path = os.path.join(company_path, file_name)
        if os.path.isfile(file_path):
            df = process_file(file_path, company_name, model, tokenizer, temp_dir, include_raw_response)
            if not df.empty:
                results.append(df)
    if results:
        combined = pd.concat(results, ignore_index=True)
        print(f"\n\n=== Results for {company_name} ===")
        print(combined.to_string(index=False))
        logger.info(f"Processed {len(results)} files for {company_name} with {len(combined)} total entries.")
        return combined
    else:
        logger.info(f"No processable information found for {company_name}.")
        return pd.DataFrame()

if __name__ == "__main__":
    #if '-f' in sys.argv: sys.argv.remove('-f')
    # --- Manual Control for Testing in Jupyter/IPython ---
    # To process a specific company, uncomment and set the name below.
    # To process all companies, leave it commented out or set to ['script.py'].
    
    # Example for testing a single company:
    #sys.argv = ['script.py', '10 - Hume 88 Pty Ltd']
    
    # Example for processing all companies (if you want to run the full set):
    sys.argv = ['script.py']
    
    # ---------------------------------------------------------

    parser = argparse.ArgumentParser(description="Process sales information from financial documents.")
    parser.add_argument("company", type=str, nargs='?', default=None, help="Specify a single company folder to process.")
    
    # Use parse_known_args() to gracefully ignore any extra arguments from the environment
    args, unknown = parser.parse_known_args()
    if unknown:
        logger.warning(f"Ignoring unknown command-line arguments: {unknown}")

    model, tokenizer = initialize_model()
    if not (model and tokenizer):
        logger.error("Model initialization failed. Exiting.")
        sys.exit(1)

    # Use a try...finally block to ensure temp directory is always cleaned up
    temp_dir = tempfile.mkdtemp()
    try:
        logger.info(f"Extracting archive {ARCHIVE_PATH} to {temp_dir}")
        extract_archive(ARCHIVE_PATH, temp_dir)
        
        # The archive often extracts into a single root folder. Let's use that.
        # This makes the script more robust if the archive has a top-level directory.
        extracted_contents = os.listdir(temp_dir)
        if len(extracted_contents) == 1 and os.path.isdir(os.path.join(temp_dir, extracted_contents[0])):
            processing_root = os.path.join(temp_dir, extracted_contents[0])
            logger.info(f"Processing content within extracted folder: {processing_root}")
        else:
            processing_root = temp_dir

        if args.company:
            logger.info(f"Processing single company: {args.company}")
            company_df = process_company_folder(args.company, model, tokenizer, processing_root, include_raw_response=True)
            if not company_df.empty:
                company_df.to_csv(OUTPUT_CSV, index=False)
                logger.info(f"Single company processing complete. Saved results to {OUTPUT_CSV}")
        else:
            logger.info("Processing all companies in the archive...")
            all_results = []
            for company_folder in sorted(os.listdir(processing_root)):
                company_path = os.path.join(processing_root, company_folder)
                if os.path.isdir(company_path):
                    company_df = process_company_folder(company_folder, model, tokenizer, processing_root, include_raw_response=True)
                    if not company_df.empty:
                        all_results.append(company_df)
            
            if all_results:
                final_df = pd.concat(all_results, ignore_index=True)
                print("\n\n=== FINAL COMBINED RESULTS ===")
                print(final_df.to_string())
                final_df.to_csv(OUTPUT_CSV, index=False)
                logger.info(f"Processing complete. Saved {len(final_df)} total entries to {OUTPUT_CSV}")
            else:
                logger.info("Processing complete. No information was extracted from any company.")

    except Exception as e:
        logger.critical(f"An unhandled error occurred in the main execution block: {e}", exc_info=True)
    finally:
        logger.info(f"Cleaning up temporary directory: {temp_dir}")
        shutil.rmtree(temp_dir, ignore_errors=True)
        logger.info("Script finished.")
