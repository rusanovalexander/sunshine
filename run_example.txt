, this behavior change is almost certainly due to updates in the transformers and accelerate libraries, combined with how the model is being loaded (specifically with quantization).
Here is the detailed breakdown of why this is happening and why it is so much slower.
1. The difference between "Tensors/Shards" and "Weights/Materializing"
Before ("Loading checkpoint shards 0/8"):
This indicates that the library was loading file chunks (shards). It was reading large .safetensors or .bin files directly into memory and mapping them to the GPU. This is usually fast.
Now ("Materializing param... 34/443"):
This message comes from the accelerate library. It indicates that instead of loading whole files, the system has created an "empty" skeleton of the model (on the "meta" device) and is now initializing (materializing) every single specific parameter (layer by layer, weight by weight) individually.
2. Why did it switch to this mode?
The switch usually happens due to device_map="auto" combined with Quantization (bitsandbytes) in newer library versions.
When you use 4-bit loading (load_in_4bit=True), transformers delegates the loading process to accelerate.
The "Meta" Device: To save CPU RAM, accelerate creates the model on a "meta" device (it takes up 0 RAM).
Materialization: It then iterates through every named parameter (e.g., model.layers.2.self_attn.q_proj.weight), finds that data in the disk files, loads just that piece, quantizes it (if necessary), and moves it to the GPU.
Why is it happening now?
Missing SafeTensors: If your local model folder (/home/inghero/.../qwen3_14B) contains standard PyTorch .bin files (pickled) but does not contain .safetensors files, or if the model.safetensors.index.json is missing/mismatched, accelerate is forced to take this slow, granular path to ensure it doesn't crash your system RAM.
Library Update: Newer versions of transformers are more aggressive about using accelerate's low_cpu_mem_usage=True default. If the library detects that loading the full shards might overflow your CPU RAM, it switches to this slow "materialization" loop.
3. Why is it so slow? (6.05s/it)
You are seeing 6 seconds per parameter. With 443 parameters, that is roughly 45 minutes to load the model.
This extreme slowness usually indicates one of two things:
Re-pickling: It is opening and closing the large .bin checkpoint files hundreds of times to find the tiny slice of data for each specific parameter.
On-the-fly Quantization: It is loading high-precision weights and calculating the 4-bit quantization for every single layer on the CPU before moving it to the GPU.
How to fix it (Make it fast again)
You can likely return to the fast "shard" loading by doing the following:
1. Ensure safetensors is installed and used
Safetensors allows memory mapping, which makes loading individual parameters instant.
code
Bash
pip install safetensors
Check your model directory (/home/inghero/.../qwen3_14B). Does it have .safetensors files? If it only has .bin files, the loading will be slow. You should download the safetensors version of the model from HuggingFace if available.
2. Update bitsandbytes
Ensure you have the latest version, as older versions struggled with the new accelerate loading pipeline.
code
Bash
pip install -U bitsandbytes
3. Modify your loader code (src.main)
If you have access to the code where AutoModelForCausalLM.from_pretrained is called, explicitly tell it to use safetensors:
code
Python
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    use_safetensors=True,  # Force safetensors
    # ... other params
)
Summary: The change is due to accelerate trying to save your memory by loading item-by-item, but failing to do so efficiently because it is likely reading from .bin files instead of .safetensors, causing massive disk I/O overhead.
